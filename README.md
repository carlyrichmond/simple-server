# Simple Server
Simple JavaScript server with regular output.

## Running the server

`server.js` is a simple Express server, which can be started by:

```zsh
npm install
node server.js
```

This server exposes a single endpoint `/` that returns a single log entry as a JSON object:

```json
{"event":{"transaction_id":1,"data_set":"my-logging-app"},"message":"WARN: Unable to get an interesting response"}
```

## Logstash

One you have installed Logstash, ensuring the `config/logstash.conf` is available in the `config directory`, the sample can be run using the below command:

 ```zsh
 ./logstash/bin/logstash -f config/logstash.conf
 ```

 To ensure a successful start, you need to specify your Elasticsearch credentials in the `logstash.yml` file, and expose your `ELASTIC_CLOUD_ID` and `ELASTIC_CLOUD_AUTH` settings as environment variables to be picked up by your Logstash process.

The `logstash.conf` file shows 3 different approaches to managing log events ingested into Elasticsearch using Logstash.

### Option 1: Ingest each event

The first approach, which is the default behaviour, is where a document `_id` is generated by Elasticsearch on ingestion of each document:

```yml
elasticsearch { 
    cloud_id => "${ELASTIC_CLOUD_ID}" 
    cloud_auth => "${ELASTIC_CLOUD_AUTH}"
    # Option 1: Elasticsearch generated ID
    index => "my-logstash-index"
}
```

### Option 2: Specifying `event.transaction_id` as document ID

For log entries that have a unique ID, specifying the field using the `document_id` will mean this value is used, and Elasticsearch will check for this `_id` on ingest of every document.

```yml
elasticsearch { 
    cloud_id => "${ELASTIC_CLOUD_ID}" 
    cloud_auth => "${ELASTIC_CLOUD_AUTH}"
    
    # Option 2: document_id
    index => "my-unique-logstash-index"
    document_id => "%{[event][transaction_id]}"
}
```

### Option 3: Fingerprint-generated ID

The final way is to generate an id by hashing a set of fields that you consider relevant to generating a unique identifier. In the below example we use a combination of `event.start_date`, `event.data_set` and `message` to generate the ID using SHA-256 hashing algorithm.

```yml
 filter {
  fingerprint {
    source => ["event.start_date", "event.data_set", "message"]
    target => "[@metadata][fingerprint]"
    method => "SHA256"
  }
}
output {
  elasticsearch { 
    cloud_id => "${ELASTIC_CLOUD_ID}" 
    cloud_auth => "${ELASTIC_CLOUD_AUTH}"
    
    # Option 3: fingerprint
    index => "my-fingerprinted-logstash-index"
    document_id => "%{[@metadata][fingerprint]}" 
}
```